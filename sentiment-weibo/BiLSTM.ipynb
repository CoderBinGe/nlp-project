{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config ZMQInteractiveShell.ast_node_interactivity='all'\n",
    "import pandas as pd\n",
    "from config import *\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 样本乱序\n",
    "def shuffles(inputs, outputs):\n",
    "    contents = []\n",
    "    f1 = open(inputs, 'r', encoding='utf-8') \n",
    "    for line in f1.readlines():\n",
    "        contents.append(line)\n",
    "    \n",
    "    random.shuffle(contents)\n",
    "    \n",
    "    f2 = open(outputs, 'w', encoding='utf-8')\n",
    "    for content in contents:\n",
    "        f2.write(content)\n",
    "    \n",
    "    f1.close()\n",
    "    f2.close()\n",
    "\n",
    "\n",
    "shuffles(inputs,outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119988, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>死于1,5,6,7,10,转发，求免死，哈哈//@Stella盟:睡眠不足死、朝八晚无死，饮...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>才说完人家旁边福特车胎补两个，我补一个。结果，原来，我的也是两个！哭！一边一个！一个钉子，一...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>#小编传送门#这个冬天，拥有一本温情的教养书，挺好的。[可爱]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[泪][衰][衰]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[泪][泪]一定会好起来的！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>一家好的公司都有严格的薪酬体系，这个很正常，并非人情味，这是你本该得到的。 //@马蝎子:来...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>和我一样无法接受的请举手……[衰]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>朦朦胧胧[抓狂]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>北广！[泪] //@自由的馨淇:我当年高考唯一的志愿就是北京广播学院，也就是现在的中国传媒大...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>打开电视，各台主要剧种：抗日战争，宫廷争斗，婆媳关系，正室小三，外加寒暑假的还珠格格和西游记...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                                  1\n",
       "0  1  死于1,5,6,7,10,转发，求免死，哈哈//@Stella盟:睡眠不足死、朝八晚无死，饮...\n",
       "1  0  才说完人家旁边福特车胎补两个，我补一个。结果，原来，我的也是两个！哭！一边一个！一个钉子，一...\n",
       "2  1                    #小编传送门#这个冬天，拥有一本温情的教养书，挺好的。[可爱]\n",
       "3  0                                          [泪][衰][衰]\n",
       "4  0                                     [泪][泪]一定会好起来的！\n",
       "5  0  一家好的公司都有严格的薪酬体系，这个很正常，并非人情味，这是你本该得到的。 //@马蝎子:来...\n",
       "6  0                                  和我一样无法接受的请举手……[衰]\n",
       "7  0                                           朦朦胧胧[抓狂]\n",
       "8  0  北广！[泪] //@自由的馨淇:我当年高考唯一的志愿就是北京广播学院，也就是现在的中国传媒大...\n",
       "9  1  打开电视，各台主要剧种：抗日战争，宫廷争斗，婆媳关系，正室小三，外加寒暑假的还珠格格和西游记..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 119988 entries, 0 to 119987\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   0       119988 non-null  object\n",
      " 1   1       119988 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(outputs, header=None, sep=',',encoding='utf-8')\n",
    "data.shape\n",
    "data.head(10)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"死于1',\n",
       " '才说完人家旁边福特车胎补两个，我补一个。结果，原来，我的也是两个！哭！一边一个！一个钉子，一个刀片！[泪][泪][泪][晕][晕][汗][汗][怒]\\n',\n",
       " '#小编传送门#这个冬天，拥有一本温情的教养书，挺好的。[可爱]\\n',\n",
       " '[泪][衰][衰]\\n',\n",
       " '[泪][泪]一定会好起来的！\\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 0, 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(\"\\//@[a-zA-Z\\W+]+\", \"\",text) # re.sub(pattern, repl, string, count=0, flags=0)\n",
    "    text = re.sub(\"\\@[a-zA-Z\\w+]+\", \"\",text)\n",
    "    text = re.sub(\"[\\-\\#+\\//@.\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、\"\"【】~@#￥%……&*（）]+\",\"\",text)\n",
    "    text = re.sub('[A-Za-z0-9\\!\\%\\[\\]\\,\\。\\:\\::\\?\\“\\”\\”“\\～+\\:?\\;;\\>>]','',text)\n",
    "    text = re.sub('\\：：?','',text)\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    text = re.sub(r'#{1,}', '', text)\n",
    "    return text\n",
    "\n",
    "def get_train_data():\n",
    "    texts = []\n",
    "    labels = []\n",
    "    with open(outputs,'r',encoding = 'utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            items = line.split(',')\n",
    "            texts.append(items[1])\n",
    "\n",
    "            text = items[1]\n",
    "            text = clean_text(text)\n",
    "            if len(text) > 1 and text!='\\n':\n",
    "                if items[0] == '1':\n",
    "                    labels.append(1)\n",
    "                else:\n",
    "                    labels.append(0)  # labels 这里一定要存放数字，不能是字符串'1'或'0'\n",
    "    return texts,labels\n",
    "\n",
    "texts,labels = get_train_data()\n",
    "texts[:5]\n",
    "labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载预训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "w2v_model = KeyedVectors.load_word2vec_format('./embeddings/sgns.zhihu.bigram',binary=False, unicode_errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\zhoubin\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.040 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "耗时:0.58 mins\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import time\n",
    "\n",
    "def get_train_tokens(texts, w2v_model):\n",
    "    train_tokens = []\n",
    "    for text in texts:\n",
    "        text = clean_text(text)\n",
    "        if len(text) > 1 and text!='\\n' and text!='\\t':\n",
    "            words_list = [i for i in jieba.cut(text)]\n",
    "            for i, word in enumerate(words_list):\n",
    "                try:\n",
    "                    words_list[i] = w2v_model.vocab[word].index\n",
    "                except KeyError:\n",
    "                    words_list[i] = 0\n",
    "            train_tokens.append(words_list)\n",
    "    return train_tokens\n",
    "\n",
    "start = time.time()\n",
    "train_tokens = get_train_tokens(texts, w2v_model) # [[],[],...]\n",
    "print(\"耗时:%0.2f mins\"%((time.time() - start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[267, 190],\n",
       " [126,\n",
       "  3701,\n",
       "  390,\n",
       "  865,\n",
       "  18549,\n",
       "  44158,\n",
       "  2840,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  299,\n",
       "  781,\n",
       "  6,\n",
       "  1,\n",
       "  18,\n",
       "  4,\n",
       "  0,\n",
       "  768,\n",
       "  506,\n",
       "  7,\n",
       "  7,\n",
       "  17583,\n",
       "  7,\n",
       "  25743,\n",
       "  0,\n",
       "  0,\n",
       "  5709,\n",
       "  5327,\n",
       "  5327,\n",
       "  3222]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def max_token(train_tokens):\n",
    "    num_tokens = [len(tokens) for tokens in train_tokens ]\n",
    "    num_tokens = np.array(num_tokens)\n",
    "    max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens) # 标准差\n",
    "    max_tokens = int(max_tokens)\n",
    "    return max_tokens\n",
    "\n",
    "max_tokens = max_token(train_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词汇表大小是： 76370\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def vocab_size(train_tokens):\n",
    "    start = time.time()\n",
    "    vocab_tokens = []\n",
    "    i = 0\n",
    "    for tokens in train_tokens:\n",
    "        for token in tokens:\n",
    "            i+=1\n",
    "            if token not in vocab_tokens:\n",
    "                vocab_tokens.append(token)\n",
    "            else:\n",
    "                pass\n",
    "    if i%1e6 == 0:\n",
    "        print(\"处理前%d的词花费的时间是:%0.2f\"%(i,(time.time() - start)/60),'mins')\n",
    "    print(\"词汇表大小是：\",len(vocab_tokens))\n",
    "    return len(vocab_tokens)\n",
    "\n",
    "num_words = vocab_size(train_tokens) # 这个计算比较花费时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.801784, -0.16534 ,  0.030508, ...,  0.106525,  0.553436,\n",
       "         0.43665 ],\n",
       "       [-0.651747,  0.53597 ,  0.340271, ...,  0.805399,  0.104593,\n",
       "         0.193694],\n",
       "       [-0.412321,  0.228261,  0.207114, ...,  0.808777,  0.056751,\n",
       "         0.452374],\n",
       "       ...,\n",
       "       [ 0.127824,  0.697852, -0.43761 , ..., -0.107803, -0.033679,\n",
       "        -0.517616],\n",
       "       [-0.100228, -0.134558, -0.352162, ...,  0.210711,  0.088362,\n",
       "        -0.357846],\n",
       "       [ 0.015363, -0.057104,  0.571133, ..., -0.518441,  0.157153,\n",
       "         0.328522]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_embedding_matrix(embedding_dim, num_words, w2v_model):\n",
    "    embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "    for i in range(num_words):\n",
    "        embedding_matrix[i,:] = w2v_model[w2v_model.index2word[i]]\n",
    "    embedding_matrix = embedding_matrix.astype('float32')\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "embedding_matrix = get_embedding_matrix(embedding_dim,num_words,w2v_model)\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119522, 56)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(119522,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def add_padding(train_tokens, labels, max_tokens, num_words):\n",
    "    train_pad = pad_sequences(train_tokens, maxlen = max_tokens, padding='pre', truncating='pre')\n",
    "    train_pad[train_pad >= num_words] = 0\n",
    "    labels = np.array(labels)\n",
    "    return train_pad,labels\n",
    "\n",
    "\n",
    "train_pad, labels = add_padding(train_tokens, labels, max_tokens, num_words)\n",
    "train_pad.shape\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.python.keras.models import save_model\n",
    "\n",
    "def model(epochs,batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words,embedding_dim,\n",
    "                        weights = [embedding_matrix],\n",
    "                        input_length = max_tokens,\n",
    "                        trainable = False))\n",
    "    model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "    model.add(LSTM(units=16, return_sequences=False))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    optimizer = Adam(lr=1e-3)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "\n",
    "    checkpoint = ModelCheckpoint(filepath = path_checkpoint, monitor='val_loss',\n",
    "                                 verbose=1, save_weights_only=True,\n",
    "                                 save_best_only=True)\n",
    "    try:\n",
    "        model.load_weights(path_checkpoint)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    earlystopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "    # 当指标停止提升时，降低学习速率。\n",
    "    lr_reduction = ReduceLROnPlateau(monitor='val_loss',factor=0.1, min_lr=1e-8, patience=0, verbose=1)\n",
    "    callbacks = [earlystopping, checkpoint,lr_reduction]\n",
    "    \n",
    "    model.fit(X_train, y_train,validation_split=0.2, epochs=epochs,batch_size=batch_size, callbacks=callbacks)\n",
    "    \n",
    "    # 模型保存\n",
    "    save_model(model,model_path)\n",
    "    # 模型评估\n",
    "    result = model.evaluate(X_test, y_test)\n",
    "    print(result)\n",
    "    print(' Accuracy is :{0:.2%}'.format(result[1]))\n",
    "    return result[1]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\tf1.0\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\tf1.0\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\tf1.0\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\tf1.0\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\tf1.0\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\tf1.0\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 56, 300)           22911000  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 56, 128)           186880    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 16)                9280      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 23,107,177\n",
      "Trainable params: 196,177\n",
      "Non-trainable params: 22,911,000\n",
      "_________________________________________________________________\n",
      "Train on 76493 samples, validate on 19124 samples\n",
      "76480/76493 [============================>.] - ETA: 0s - loss: 0.1304 - acc: 0.9559\n",
      "Epoch 00001: val_loss improved from inf to 0.12924, saving model to ./lstm_model/sentiment_checkpoint10.keras\n",
      "76493/76493 [==============================] - 547s 7ms/sample - loss: 0.1304 - acc: 0.9559 - val_loss: 0.1292 - val_acc: 0.9549\n",
      "23905/23905 [==============================] - 69s 3ms/sample - loss: 0.1188 - acc: 0.9593\n",
      "[0.11881209983973698, 0.959339]\n",
      " Accuracy is :95.93%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_pad, # array\n",
    "                                                    labels, # array\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=660)\n",
    "\n",
    "\n",
    "accuracy = model(epochs, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import load_model\n",
    "\n",
    "def prediction(text, w2v_model, vocab_size, maxlen):\n",
    "    text = clean_text(text)\n",
    "    if len(text)>1 and text!='\\n' and text!='\\t':\n",
    "        words_list = [i for i in jieba.cut(text)]\n",
    "        for i, word in enumerate(words_list):\n",
    "            try:\n",
    "                words_list[i] = w2v_model.vocab[word].index\n",
    "                if words_list[i]>= vocab_size:\n",
    "                    words_list[i] = 0\n",
    "            except KeyError:\n",
    "                words_list[i] = 0\n",
    "            \n",
    "    test_pad = pad_sequences([words_list], maxlen=maxlen, padding='pre', truncating='pre')\n",
    "    \n",
    "    # loading model\n",
    "    model = load_model(model_path)\n",
    "    result = model.predict(x=test_pad)\n",
    "    coefs = result[0][0]\n",
    "    return coefs*100  # 返回判断的阈值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1条微博是:我今天为什么要穿白长裙？[泪]还有几站该下车了！雨依然哗哗的！[抓狂] //@败家de小妞子:裙子已经湿了[泪]\n",
      "模型判断这是一条: ##负面情绪##,预测阈值是:0.14\n",
      "\n",
      "\n",
      "第2条微博是:糟糕透顶了，刚买的新手机就丢厕所里面了,想骂人呀，谁也别招惹我。\n",
      "模型判断这是一条: ##负面情绪##,预测阈值是:1.45\n",
      "\n",
      "\n",
      "第3条微博是:今天是个好日子，天气特别美好，我的心情也很好\n",
      "模型判断这是一条: ##正面情绪##,预测阈值是:50.36\n",
      "\n",
      "\n",
      "第4条微博是:新发的工资,还意外的领到了红包\n",
      "模型判断这是一条: ##负面情绪##,预测阈值是:45.43\n",
      "\n",
      "\n",
      "第5条微博是:朋友出车祸了\n",
      "模型判断这是一条: ##负面情绪##,预测阈值是:1.79\n",
      "\n",
      "\n",
      "第6条微博是:今天去出差，住豪华酒店\n",
      "模型判断这是一条: ##负面情绪##,预测阈值是:3.55\n",
      "\n",
      "\n",
      "第7条微博是:今天第一次陪朋友去逛街\n",
      "模型判断这是一条: ##正面情绪##,预测阈值是:80.56\n",
      "\n",
      "\n",
      "第8条微博是:这不科学……应该是一堆狗~//@蜘蛛3号: //@我的朋友是个呆B:QAQ//@进击的巨人官网:QAQ//@我的同事是个婊子: QAQ//@\n",
      "模型判断这是一条: ##负面情绪##,预测阈值是:3.37\n",
      "\n",
      "\n",
      "第9条微博是:我刚问了老公这个问题，宝宝出生后他会说什么，他不加思索地来了一句：“八喜，欢迎来到地球！”[汗][晕]\n",
      "模型判断这是一条: ##负面情绪##,预测阈值是:0.17\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_list= [\n",
    "    '我今天为什么要穿白长裙？[泪]还有几站该下车了！雨依然哗哗的！[抓狂] //@败家de小妞子:裙子已经湿了[泪]',\n",
    "    '糟糕透顶了，刚买的新手机就丢厕所里面了,想骂人呀，谁也别招惹我。',\n",
    "    '今天是个好日子，天气特别美好，我的心情也很好',\n",
    "    '新发的工资,还意外的领到了红包',\n",
    "    '朋友出车祸了',\n",
    "    '今天去出差，住豪华酒店',\n",
    "    '今天第一次陪朋友去逛街',\n",
    "    '这不科学……应该是一堆狗~//@蜘蛛3号: //@我的朋友是个呆B:QAQ//@进击的巨人官网:QAQ//@我的同事是个婊子: QAQ//@',\n",
    "    '我刚问了老公这个问题，宝宝出生后他会说什么，他不加思索地来了一句：“八喜，欢迎来到地球！”[汗][晕]'\n",
    "]\n",
    "\n",
    "results = []\n",
    "for text in text_list:\n",
    "    res = prediction(text, w2v_model, num_words, max_tokens)\n",
    "    results.append(res)\n",
    "\n",
    "    \n",
    "cte = ['正面情绪','负面情绪']\n",
    "for i in range(len(text_list)):\n",
    "    \n",
    "    if results[i] > 50:\n",
    "        cte_j = 0\n",
    "    else:\n",
    "        cte_j = 1\n",
    "    print(\"第%d条微博是:%s\"%(i+1,text_list[i]) )\n",
    "    print(\"模型判断这是一条: ##%s##,预测阈值是:%0.2f\"%(cte[cte_j],results[i]) )\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
