{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config ZMQInteractiveShell.ast_node_interactivity='all'\n",
    "import pandas as pd\n",
    "from config import *\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "import jieba\n",
    "from collections import Counter\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载源数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: 101058 101058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['才用就发现相机打开迟钝，半天反应不过来，有时候还会卡出去，他们又不给解决方案。', '还没穿二天就起毛了']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['0', '0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = []\n",
    "labels = []\n",
    "with open(dataSource,'r',encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        temp = line.replace('\\n', '').split(',,')\n",
    "        reviews.append(temp[0])\n",
    "        labels.append(temp[1])\n",
    "print('data:',len(reviews),len(labels))\n",
    "reviews[:2]\n",
    "labels[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\zhoubin\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.975 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: 101058 101058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['才',\n",
       "  '用',\n",
       "  '就',\n",
       "  '发现',\n",
       "  '相机',\n",
       "  '打开',\n",
       "  '迟钝',\n",
       "  '，',\n",
       "  '半天',\n",
       "  '反应',\n",
       "  '不',\n",
       "  '过来',\n",
       "  '，',\n",
       "  '有时候',\n",
       "  '还会卡',\n",
       "  '出去',\n",
       "  '，',\n",
       "  '他们',\n",
       "  '又',\n",
       "  '不',\n",
       "  '给',\n",
       "  '解决方案',\n",
       "  '。'],\n",
       " ['还', '没', '穿', '二天', '就', '起毛', '了']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = [jieba.lcut(review.replace('\\n', '')) for review in reviews]\n",
    "print('data:',len(reviews),len(labels))\n",
    "reviews[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model = Word2Vec(reviews,size=embeddingSize,\n",
    "#                      min_count=miniFreq,\n",
    "#                      window=10,\n",
    "#                      workers=multiprocessing.cpu_count(),sg=1,\n",
    "#                      iter=20)\n",
    "\n",
    "# w2v_model = Word2Vec(reviews,size=embeddingSize,\n",
    "#                      min_count=miniFreq,\n",
    "#                      window=10,\n",
    "#                      workers=10,sg=1,\n",
    "#                      iter=20)\n",
    "# w2v_model.save(w2v_model_path)\n",
    "\n",
    "# model = Word2Vec.load(w2v_model_path)\n",
    "# model.wv.vocab.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readStopWord(stopWordPath):\n",
    "    stopWordDict = {}\n",
    "    with open(stopWordPath, \"r\", encoding='utf-8') as f:\n",
    "        stopWords = f.read()\n",
    "        stopWordList = stopWords.splitlines()\n",
    "        # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
    "        stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "    return stopWordDict\n",
    "\n",
    "stopWordDict = readStopWord(stopWordPath)\n",
    "# stopWordDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "E:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:32: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "def genVocabulary():\n",
    "    \"\"\"\n",
    "    按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "    \"\"\"\n",
    "\n",
    "    # 中文\n",
    "    model = Word2Vec.load(w2v_model_path)\n",
    "\n",
    "    vocab = []\n",
    "    wordEmbedding = [] # 预训练权重矩阵\n",
    "\n",
    "    # 添加 \"pad\" 和 \"UNK\", \n",
    "    vocab.append(\"pad\")\n",
    "    wordEmbedding.append(np.zeros(embeddingSize))\n",
    "\n",
    "    vocab.append(\"UNK\")\n",
    "    wordEmbedding.append(np.random.randn(embeddingSize))\n",
    "    \n",
    "        \n",
    "    allWords = [word for review in reviews for word in review]\n",
    "    # 去掉停用词\n",
    "    subWords = [word for word in allWords if word not in stopWordDict]\n",
    "    # 统计词频，排序\n",
    "    wordCount = Counter(subWords)\n",
    "    sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "    # 去除低频词\n",
    "    words = [item[0] for item in sortWordCount if item[1] >= miniFreq]\n",
    "    \n",
    "    # 获取词列表和顺序对应的预训练权重矩阵\n",
    "    for word in words:\n",
    "        try:\n",
    "            vector = model[word]\n",
    "            vocab.append(word)\n",
    "            wordEmbedding.append(vector)\n",
    "\n",
    "        except:\n",
    "            print(word + \"不存在于词向量中\")\n",
    "            \n",
    "    \n",
    "    wordToIndex = dict(zip(vocab, list(range(len(vocab)))))\n",
    "    indexToWord = dict(zip(list(range(len(vocab))), vocab))\n",
    "    \n",
    "    # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "    with open(\"./temp/wordJson/wordToIndex.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(wordToIndex, f)\n",
    "\n",
    "    with open(\"./temp/wordJson/indexToWord.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(indexToWord, f)\n",
    "\n",
    "    return vocab, np.array(wordEmbedding), wordToIndex, indexToWord\n",
    "\n",
    "\n",
    "vocab, wordEmbedding, wordToIndex, indexToWord = genVocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (80846, 200)\n",
      "train label shape: (80846, 2)\n",
      "eval data shape: (20212, 200)\n",
      "eval label shape: (20212, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   1,    1,    1,   71,  340,   79, 1923,    1,  274,    1,    1,\n",
       "           1,    1,  260, 6621,    1,    1,    1,    1,    1,    1, 2039,\n",
       "           1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0],\n",
       "       [   1,    1,   14, 3225,    1, 3343,    1,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def reviewProcess(review, sequenceLength, wordToIndex):\n",
    "    \"\"\"\n",
    "    将数据集中的每条评论里面的词，根据词表，映射为index表示\n",
    "    每条评论 用index组成的定长数组来表示\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    reviewVec = np.zeros((sequenceLength))\n",
    "    \n",
    "    sequenceLen = sequenceLength\n",
    "\n",
    "    # 判断当前的序列是否小于定义的固定序列长度\n",
    "    if len(review) < sequenceLength:\n",
    "        sequenceLen = len(review)\n",
    "\n",
    "    for i in range(sequenceLen):\n",
    "        if review[i] in wordToIndex:\n",
    "            reviewVec[i] = wordToIndex[review[i]]\n",
    "        else:\n",
    "            reviewVec[i] = wordToIndex[\"UNK\"]\n",
    "\n",
    "    return reviewVec\n",
    "\n",
    "\n",
    "\n",
    "def genTrainEvalData(x, y, rate):\n",
    "    \"\"\"\n",
    "    生成训练集和验证集\n",
    "    \"\"\"\n",
    "\n",
    "    reviews = []\n",
    "    labels = []\n",
    "\n",
    "    # 遍历所有的文本，将文本中的词转换成index表示\n",
    "    for i in range(len(x)):\n",
    "        reviewVec = reviewProcess(x[i], sequenceLength, wordToIndex)\n",
    "        reviews.append(reviewVec)\n",
    "        labels.append([y[i]])\n",
    "\n",
    "    trainIndex = int(len(x) * rate)\n",
    "\n",
    "#     trainReviews = pad_sequences(reviews[:trainIndex], maxlen=sequenceLength)\n",
    "    trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "    trainLabels = np.array(labels[:trainIndex], dtype=\"float32\")\n",
    "    \n",
    "    trainLabels = to_categorical(trainLabels,num_classes=2)\n",
    "\n",
    "#     evalReviews = pad_sequences(reviews[trainIndex:], maxlen=sequenceLength)\n",
    "    evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "    evalLabels = np.array(labels[trainIndex:], dtype=\"float32\")\n",
    "    \n",
    "    evalLabels = to_categorical(evalLabels,num_classes=2)\n",
    "\n",
    "    return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "\n",
    "\n",
    "trainReviews, trainLabels, evalReviews, evalLabels = genTrainEvalData(reviews, labels, rate=rate)\n",
    "print(\"train data shape: {}\".format(trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(evalReviews.shape))\n",
    "print(\"eval label shape: {}\".format(evalLabels.shape))\n",
    "trainReviews[:2]\n",
    "trainLabels[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D,GlobalAveragePooling1D,concatenate,Flatten,Dense,Dropout,Embedding,Reshape,LSTM,Layer,Bidirectional,GRU,BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import Sequential,optimizers,losses\n",
    "from tensorflow.keras import backend\n",
    "backend.clear_session() \n",
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "\n",
    "class Position_Embedding(Layer):\n",
    "    def __init__(self, size=None, mode='sum', **kwargs):\n",
    "        self.size = size #必须为偶数\n",
    "        self.mode = mode\n",
    "        super(Position_Embedding, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        if (self.size == None) or (self.mode == 'sum'):\n",
    "            self.size = int(x.shape[-1])\n",
    "        batch_size,seq_len = K.shape(x)[0],K.shape(x)[1]\n",
    "        position_j = 1. / K.pow(10000., 2 * K.arange(self.size / 2, dtype='float32') / self.size)\n",
    "        position_j = K.expand_dims(position_j, 0)\n",
    "        position_i = K.cumsum(K.ones_like(x[:,:,0]), 1)-1 #K.arange不支持变长，只好用这种方法生成\n",
    "        position_i = K.expand_dims(position_i, 2)\n",
    "        position_ij = K.dot(position_i, position_j)\n",
    "        position_ij = K.concatenate([K.cos(position_ij), K.sin(position_ij)], 2)\n",
    "        if self.mode == 'sum':\n",
    "            return position_ij + x\n",
    "        elif self.mode == 'concat':\n",
    "            return K.concatenate([position_ij, x], 2)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.mode == 'sum':\n",
    "            return input_shape\n",
    "        elif self.mode == 'concat':\n",
    "            return (input_shape[0], input_shape[1], input_shape[2]+self.size)\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, nb_head, size_per_head, **kwargs):\n",
    "        self.nb_head = nb_head\n",
    "        self.size_per_head = size_per_head\n",
    "        self.output_dim = nb_head*size_per_head\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.WQ = self.add_weight(name='WQ',\n",
    "                                  shape=(input_shape[0][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WK = self.add_weight(name='WK',\n",
    "                                  shape=(input_shape[1][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WV = self.add_weight(name='WV',\n",
    "                                  shape=(input_shape[2][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def Mask(self, inputs, seq_len, mode='mul'):\n",
    "        if seq_len == None:\n",
    "            return inputs\n",
    "        else:\n",
    "            mask = K.one_hot(seq_len[:,0], K.shape(inputs)[1])\n",
    "            mask = 1 - K.cumsum(mask, 1)\n",
    "            for _ in range(len(inputs.shape)-2):\n",
    "                mask = K.expand_dims(mask, 2)\n",
    "            if mode == 'mul':\n",
    "                return inputs * mask\n",
    "            if mode == 'add':\n",
    "                return inputs - (1 - mask) * 1e12\n",
    "\n",
    "    def call(self, x):\n",
    "        #如果只传入Q_seq,K_seq,V_seq，那么就不做Mask\n",
    "        #如果同时传入Q_seq,K_seq,V_seq,Q_len,V_len，那么对多余部分做Mask\n",
    "        if len(x) == 3:\n",
    "            Q_seq,K_seq,V_seq = x\n",
    "            Q_len,V_len = None,None\n",
    "        elif len(x) == 5:\n",
    "            Q_seq,K_seq,V_seq,Q_len,V_len = x\n",
    "        #对Q、K、V做线性变换\n",
    "        Q_seq = K.dot(Q_seq, self.WQ)\n",
    "        Q_seq = K.reshape(Q_seq, (-1, K.shape(Q_seq)[1], self.nb_head, self.size_per_head))\n",
    "        Q_seq = K.permute_dimensions(Q_seq, (0,2,1,3))\n",
    "        K_seq = K.dot(K_seq, self.WK)\n",
    "        K_seq = K.reshape(K_seq, (-1, K.shape(K_seq)[1], self.nb_head, self.size_per_head))\n",
    "        K_seq = K.permute_dimensions(K_seq, (0,2,1,3))\n",
    "        V_seq = K.dot(V_seq, self.WV)\n",
    "        V_seq = K.reshape(V_seq, (-1, K.shape(V_seq)[1], self.nb_head, self.size_per_head))\n",
    "        V_seq = K.permute_dimensions(V_seq, (0,2,1,3))\n",
    "        #计算内积，然后mask，然后softmax\n",
    "        A = K.batch_dot(Q_seq, K_seq, axes=[3,3]) / self.size_per_head**0.5\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))\n",
    "        A = self.Mask(A, V_len, 'add')\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))\n",
    "        A = K.softmax(A)\n",
    "        #输出并mask\n",
    "        O_seq = K.batch_dot(A, V_seq, axes=[3,2])\n",
    "        O_seq = K.permute_dimensions(O_seq, (0,2,1,3))\n",
    "        O_seq = K.reshape(O_seq, (-1, K.shape(O_seq)[1], self.output_dim))\n",
    "        O_seq = self.Mask(O_seq, Q_len, 'mul')\n",
    "        return O_seq\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1], self.output_dim)\n",
    "    \n",
    "    \n",
    "    \n",
    "def transfromer(max_features, embedding_weights, maxlen):\n",
    "    S_inputs = Input(shape=(None,), dtype='int32')\n",
    "\n",
    "    embeddings = Embedding(input_dim=max_features, output_dim=embeddingSize,\n",
    "                            weights=[embedding_weights],\n",
    "                            input_length=maxlen)(S_inputs)\n",
    "\n",
    "    #增加Position_Embedding能轻微提高准确率\n",
    "    embeddings = Position_Embedding()(embeddings) \n",
    "\n",
    "    O_seq = Attention(8,16)([embeddings,embeddings,embeddings])\n",
    "\n",
    "    O_seq = GlobalAveragePooling1D()(O_seq)\n",
    "\n",
    "    O_seq = Dropout(dropoutKeepProb)(O_seq)\n",
    "\n",
    "    outputs = Dense(2, activation='softmax')(O_seq)\n",
    "\n",
    "    model = Model(inputs=S_inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "构建模型...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in converted code:\n\n    <ipython-input-8-8c9619d5189e>:94 call  *\n        A = K.permute_dimensions(A, (0,3,2,1))\n    E:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py:2768 permute_dimensions\n        return array_ops.transpose(x, perm=pattern)\n    E:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py:1870 transpose\n        ret = transpose_fn(a, perm, name=name)\n    E:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py:11454 transpose\n        \"Transpose\", x=x, perm=perm, name=name)\n    E:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py:793 _apply_op_helper\n        op_def=op_def)\n    E:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py:548 create_op\n        compute_device)\n    E:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:3429 _create_op_internal\n        op_def=op_def)\n    E:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1773 __init__\n        control_input_ops)\n    E:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1613 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Dimension must be 5 but is 4 for 'attention/transpose_7' (op: 'Transpose') with input shapes: [?,8,?,8,?], [4].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a7a7d6a23c62>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'构建模型...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransfromer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordEmbedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequenceLength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-8c9619d5189e>\u001b[0m in \u001b[0;36mtransfromer\u001b[1;34m(max_features, embedding_weights, maxlen)\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPosition_Embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m     \u001b[0mO_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAttention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[0mO_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGlobalAveragePooling1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mO_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    840\u001b[0m                     not base_layer_utils.is_in_eager_or_tf_function()):\n\u001b[0;32m    841\u001b[0m                   \u001b[1;32mwith\u001b[0m \u001b[0mauto_control_deps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAutomaticControlDependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0macd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 842\u001b[1;33m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    843\u001b[0m                     \u001b[1;31m# Wrap Tensors in `outputs` in `tf.identity` to avoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    844\u001b[0m                     \u001b[1;31m# circular dependencies.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    235\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in converted code:\n\n    <ipython-input-8-8c9619d5189e>:94 call  *\n        A = K.permute_dimensions(A, (0,3,2,1))\n    E:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py:2768 permute_dimensions\n        return array_ops.transpose(x, perm=pattern)\n    E:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py:1870 transpose\n        ret = transpose_fn(a, perm, name=name)\n    E:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py:11454 transpose\n        \"Transpose\", x=x, perm=perm, name=name)\n    E:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py:793 _apply_op_helper\n        op_def=op_def)\n    E:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py:548 create_op\n        compute_device)\n    E:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:3429 _create_op_internal\n        op_def=op_def)\n    E:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1773 __init__\n        control_input_ops)\n    E:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1613 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Dimension must be 5 but is 4 for 'attention/transpose_7' (op: 'Transpose') with input shapes: [?,8,?,8,?], [4].\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import yaml\n",
    "\n",
    "x_train = trainReviews\n",
    "y_train = trainLabels\n",
    "x_eval = evalReviews\n",
    "y_eval = evalLabels\n",
    "\n",
    "max_features = len(wordToIndex) + 1\n",
    "\n",
    "print('构建模型...')\n",
    "model = transfromer(max_features, wordEmbedding, sequenceLength)\n",
    "model.summary()\n",
    "\n",
    "# 回调函数\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=10, mode='auto')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "model_checkpoint = ModelCheckpoint('./model/transform_model/model_{epoch:02d}-{val_accuracy:.2f}.hdf5',\n",
    "                                   save_best_only=True, save_weights_only=True)\n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=batchSize, epochs=epochs, validation_split=0.3,\n",
    "                    shuffle=True, callbacks=[reduce_lr,early_stopping,model_checkpoint])\n",
    "#验证\n",
    "scores = model.evaluate(x_eval, y_eval)\n",
    "\n",
    "#保存模型\n",
    "yaml_string = model.to_yaml()\n",
    "with open('./model/transfromer.yml', 'w') as outfile:\n",
    "    outfile.write( yaml.dump(yaml_string, default_flow_style=True))\n",
    "model.save_weights('./model/transfromer.h5')\n",
    "\n",
    "print('test_loss: %f, accuracy: %f' % (scores[0], scores[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()   # clear figure\n",
    "acc_values = history_dict['accuracy']\n",
    "val_acc_values = history_dict['val_accuracy']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "164.988px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
